# Simplified single-file Hydra configuration

# Global settings
seed: 42
log_level: INFO

# Lightning module and data module targets
module:
  _target_: sam2_video.training.trainer.SAM2LightningModule
  # Pass unpacked sections instead of a single config
  model: ${model}
  loss: ${loss}
  optimizer: ${optimizer}
  scheduler: ${scheduler}
  visualization: ${visualization}

data_module:
  _target_: sam2_video.training.trainer.SAM2LightningDataModule
  data: ${data}

# Model configuration
model:
  _target_: sam2_video.model.sam2model.SAM2Model
  # Paths
  checkpoint_path: /bd_byta6000i0/users/surgicaldinov2/kyyang/sam2/checkpoints/sam2.1_hiera_tiny.pt
  # Resolve relative to original working directory so Hydra's output dir change doesn't break paths
  config_path: sam2/sam2.1_hiera_t.yaml

  # Trainable modules
  trainable_modules:
    - memory_attention
    - memory_encoder

  # Device and image
  device: cuda
  image_size: 512

  # Prompting behavior
  prompt_type: point  # {point, box, mask}
  forward_backbone_per_frame_for_eval: false

  # Prompt generation parameters
  num_pos_points: 2
  num_neg_points: 0
  include_center: true

# Data configuration (COCO-style)
data:
  train_path: /bd_byta6000i0/users/surgicaldinov2/kyyang/sam2/cholecseg8k/coco_style/merged_gt_coco_annotations_train.json
  val_path: /bd_byta6000i0/users/surgicaldinov2/kyyang/sam2/cholecseg8k/coco_style/merged_gt_coco_annotations_test.json
  image_size: 512
  video_clip_length: 8
  stride: 4

  num_workers: 4
  batch_size: 1
  num_categories: 13

# Loss weights
loss:
  # Select loss: {multi_step, bce}
  type: multi_step
  # Subsample frames for loss (use frames 0, k, 2k, ...)
  gt_stride: 1
  # Multi-step weights (used when type=multi_step)
  bce_weight: 1.0
  dice_weight: 1.0
  iou_weight: 0.5
  # BCE-only options (used when type=bce)
  # bce_pos_weight: null  # e.g., [1.0, 2.0, ...] length == num_categories
  # bce_reduction: mean

# Optimizer configuration
optimizer:
  type: AdamW
  lr: 1e-4
  weight_decay: 1e-4
  betas: [0.9, 0.999]
  eps: 1e-8

# Scheduler configuration
scheduler:
  enabled: true
  type: CosineAnnealingLR
  eta_min: 1e-6
  T_max: null

# Trainer configuration
trainer:
  _target_: lightning.pytorch.trainer.trainer.Trainer
  # Common Trainer arguments
  accelerator: auto
  devices: 1
  precision: 32
  max_epochs: 5
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  val_check_interval: 1.0
  num_sanity_val_steps: 2
  enable_checkpointing: true
  enable_progress_bar: true
  log_every_n_steps: 20


# SwanLab logger configuration
swanlab:
  _target_: swanlab.integration.pytorch_lightning.SwanLabLogger
  project: sam2-video-training
  # The following are provided at runtime from train.py:
  # experiment_name, description, config, logdir

# Optional Weights & Biases logger
wandb:
  _target_: lightning.pytorch.loggers.wandb.WandbLogger
  project: sam2-video-training
  name: null  # default to auto-generated
  save_dir: ${hydra:run.dir}/logs
  log_model: false
  tags: []

# Callbacks (ModelCheckpoint + LR monitor)
callbacks:
  - _target_: lightning.pytorch.callbacks.ModelCheckpoint
    monitor: val/total_loss
    mode: min
    save_top_k: 3
    save_last: true
    dirpath: ${hydra:run.dir}/checkpoints
    filename: sam2-epoch{epoch:02d}-val_loss{val/total_loss:.4f}

  - _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: step
  
  - _target_: lightning.pytorch.callbacks.ModelSummary  
    max_depth: 2

  - _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: "val/total_loss"
    patience: 5
    mode: min

# Visualization controls
visualization:
  # Conservative defaults to minimize training overhead
  enabled: true
  train_every_n_steps: 20  # Disabled by default
  val_first_batch_every_n_epochs: 1
  max_length: 4
  stride: 1
  caption: "cholecseg8k"
